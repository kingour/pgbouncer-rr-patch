diff --git a/src/server.c b/src/server.c
index c6e5efe..1d7a766 100644
--- a/src/server.c
+++ b/src/server.c
@@ -27,6 +27,125 @@
 
 #define ERRCODE_CANNOT_CONNECT_NOW "57P03"
 
+bool routing_traffic_to_target_complete = false;
+bool source_instance_down_before_switchover = false;
+/* Stores the new port number set by customer for the green (target) instance during Blue-Green Deployment. */
+int updated_target_port_num = 0;
+
+/**
+ * Extracts and processes query result data from a packet into a tab-separated string.
+ *
+ * This function performs the following steps:
+ * 1. Reads the number of columns from the packet header
+ * 2. Calculates total required buffer length in first pass
+ * 3. Allocates memory for output string
+ * 4. Copies column data with tab separators in second pass
+ *
+ * The returned string format is: col1\tcol2\tcol3...\tcolN\0
+ *
+ * @param pkt       Pointer to PktHdr structure containing raw packet data
+ *                  Must contain valid column count and column data
+ *
+ * @return          On success: Pointer to newly allocated string containing tab-separated column data
+ *                  On failure: NULL (with error logged)
+ *                  Caller must free the returned string
+ *
+ * @note           Function will return NULL and log error if:
+ *                 - Cannot read column count
+ *                 - Zero columns in packet
+ *                 - Cannot read column lengths or data
+ *                 - Memory allocation fails
+ *                 - Final position doesn't match calculated length
+ *
+ * @warning        The returned string must be freed by the caller to avoid memory leaks
+ */
+
+static char *query_data(PktHdr *pkt)
+{
+	uint16_t columns;
+	uint32_t length;
+	const char *data;
+	char *output = NULL;
+	size_t total_length = 0;
+	size_t pos = 0;
+	int i;
+	struct MBuf tmp_buf;
+	const uint8_t *dummy;
+
+	/* Get number of columns */
+	if (!mbuf_get_uint16be(&pkt->data, &columns)) {
+		log_error("could not get packet column count");
+		return NULL;
+	}
+
+	if (columns == 0) {
+		log_error("zero columns in packet");
+		return NULL;
+	}
+
+	/* First pass: calculate total length needed */
+	mbuf_copy(&pkt->data, &tmp_buf);  /* Create a copy of the buffer */
+	for (i = 0; i < columns; i++) {
+		if (!mbuf_get_uint32be(&tmp_buf, &length)) {
+			log_error("could not get column %d length", i);
+			return NULL;
+		}
+
+		total_length += length;
+
+		if (!mbuf_get_bytes(&tmp_buf, length, &dummy)) {
+			log_error("could not skip column %d data", i);
+			return NULL;
+		}
+	}
+
+	/* Account for tabs between columns (columns - 1) and 1 null terminator */
+	total_length += (columns - 1) + 1;
+
+	/* Allocate buffer */
+	output = malloc(total_length);
+	if (output == NULL) {
+		log_error("malloc failed: no memory in query_data");
+		return NULL;
+	}
+
+	/* Second pass: actually fetch the data */
+	for (i = 0; i < columns; i++) {
+		if (!mbuf_get_uint32be(&pkt->data, &length)) {
+			log_error("could not get column %d length (second pass)", i);
+			free(output);
+			return NULL;
+		}
+
+		if (!mbuf_get_chars(&pkt->data, length, &data)) {
+			log_error("could not get column %d data (second pass)", i);
+			free(output);
+			return NULL;
+		}
+
+		/* Copy column data */
+		memcpy(output + pos, data, length);
+		pos += length;
+
+		/* Add separator (tab or null terminator) */
+		if (i < columns - 1) {
+			output[pos++] = '\t';
+		} else {
+			output[pos++] = '\0';  /* final null terminator */
+		}
+	}
+
+	if (pos != total_length) {
+		free(output);
+		fatal("final position (%zu) does not match calculated total_length (%zu)", pos, total_length);
+	}
+
+	log_debug("Extracted data: [%s], total_length: %zu, final_pos: %zu", output, total_length, pos);
+
+	return output;
+}
+
+
 static bool load_parameter(PgSocket *server, PktHdr *pkt, bool startup)
 {
 	const char *key, *val;
@@ -115,6 +234,11 @@ const char * kill_pool_logins_server_error(PgPool *pool, PktHdr *errpkt)
 	return msg;
 }
 
+static bool should_reset_global_writer(PgSocket *server) {
+	return fast_switchover && server->pool->db->recovery_query &&
+	(source_instance_down_before_switchover || routing_traffic_to_target_complete);
+}
+
 /* process packets on server auth phase */
 static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 {
@@ -122,6 +246,9 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 	const char *msg;
 	bool res = false;
 	const uint8_t *ckey;
+	char *data = NULL;
+	TopologyData *topology = NULL;
+	const char *topology_parser_msg;
 
 	if (incomplete_pkt(pkt)) {
 		disconnect_server(server, true, "partial pkt in login phase");
@@ -133,12 +260,31 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 		switch (pkt->type) {
 		case PqMsg_ReadyForQuery:
 		case PqMsg_ParameterStatus:
+		case PqMsg_DataRow:
 			/* handle them below */
 			break;
 
 		case PqMsg_ErrorResponse:
 			/* log & ignore errors */
 			log_server_error("S: error while executing exec_on_query", pkt);
+			/*
+			 * require topology table to exist in the cluster if using fast switchover.
+			 */
+			if (fast_switchover && server->pool->db->recovery_query && is_cluster_endpoint(server->pool->db->host)) {
+				/*
+				 * During blue-green deployment, the fast switchover feature's topology query may fail if the cluster is not part of the deployment.
+				 * To handle this scenario, if the topology query encounters an error, PgBouncer will not crash; instead, it will disable the fast switchover feature.
+				 * Once the customer sets up the blue-green deployment and performs a RELOAD, the fast switchover feature will be re-enabled
+				 * if the topology query successfully finds the node during execution.
+				 */
+				log_warning("Fast switchover disabled: topology query failed. The current cluster node may not be part of a Blue-Green deployment. "
+					"Feature will be re-enabled after a successful topology query following RELOAD.");
+				fast_switchover = false;
+				is_topology_monitoring_job_enabled = false;
+			} else if (fast_switchover) {
+				fatal("does the topology table exist?");
+			}
+
 		/* fallthrough */
 		default:	/* ignore rest */
 			sbuf_prepare_skip(sbuf, pkt->len);
@@ -152,6 +298,53 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 		disconnect_server(server, true, "unknown pkt from server");
 		break;
 
+	case PqMsg_DataRow:
+		if (fast_switchover && server->pool->db->topology_query && server->pool->initial_writer_endpoint) {
+			data = query_data(pkt);
+			if (data) {
+				/* Parse topology data from a query result. */
+				topology_parser_msg = parse_topology_data(server, data, &topology);
+				if (topology_parser_msg) {
+					free(data);
+					fatal("failed while parsing the topology data. Error msg : %s", topology_parser_msg);
+				}
+
+				if (server->pool->db->recovery_query) {
+					/*
+					 * Blue-Green Deployment Fast Switchover:
+					 * Open connections only to the specified endpoint and its green counterpart; skip all other endpoints.
+					 * Connection Pool Handling:
+					 * - For writer endpoints (e.g., used with PgBouncer), ensure a connection is established to the corresponding writer on the green cluster.
+					 * - For reader endpoints, establish a connection to the equivalent reader on the green cluster.
+					 * - Apply the same logic for custom-defined endpoints.
+					 */
+					bool should_continue = setup_connection_pool_based_on_endpoint_type(server->pool->db->host, &topology->endpoint);
+					if (!should_continue) {
+						free(data);
+						cleanup_topology(topology);
+						sbuf_prepare_skip(sbuf, pkt->len);
+						return true;
+					}
+				}
+
+				/* Set up a new connection pool based on topology data. */
+				if (setup_new_pool(server->pool, topology) == NULL) {
+					// Handle setup failure if needed
+					log_debug("Failed to initialize new connection pool for host '%s'. Possible issue with memory allocation, hostname parsing, or connection launch.",
+						topology->endpoint);
+					cleanup_topology(topology);
+					free(data);
+					fatal("Failed to set up connection pool. Aborting to prevent inconsistent topology state.");
+
+				}
+				cleanup_topology(topology);
+				free(data);
+			}
+		}
+
+		sbuf_prepare_skip(sbuf, pkt->len);
+		return true;
+
 	case PqMsg_ErrorResponse:
 		/*
 		 * If we cannot log into the server, then we drop all clients
@@ -189,7 +382,6 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 	case PqMsg_ParameterStatus:
 		res = load_parameter(server, pkt, true);
 		break;
-
 	case PqMsg_ReadyForQuery:
 		if (server->exec_on_connect) {
 			server->exec_on_connect = false;
@@ -201,8 +393,45 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 			if (!res)
 				disconnect_server(server, false, "exec_on_connect query failed");
 			break;
+		} else if (fast_switchover && server->pool->db->topology_query && server->pool->initial_writer_endpoint) {
+			server->exec_on_connect = true;
+			slog_debug(server, "server connect ok, send topology_query: %s", server->pool->db->topology_query);
+			SEND_generic(res, server, PqMsg_Query, "s", server->pool->db->topology_query);
+			if (!res)
+				disconnect_server(server, false, "exec_on_connect query failed");
+			break;
+		} else if (should_reset_global_writer(server)) {
+			if (server->pool->db->blue_green_deployment_db_type == DEFAULT) {
+				/*
+				 * This scenario occurs when the source instance was down before the switchover and has now become available.
+				 * During the handle server work, we monitor the metadata status. If the metadata entry shows AVAILABLE,
+				 * it indicates that the database connection was not dropped due to the switchover.
+				 * In this case, we wait for the source instance to come back online. Once it is available,
+				 * we reset the global writer to the source instance.
+				 */
+				log_debug("Setting the global writer once the default pool comes up after shut down: db_name %s", server->pool->db->name);
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+			}
 		}
 
+		if (server->pool->db->topology_query && server->pool->initial_writer_endpoint && server->pool->num_nodes < 2) {
+			if (server->pool->db->recovery_query) {
+				/*
+				 * If Blue-Green Deployment fast switchover is configured on an instance/cluster node without actually creating a Blue-Green Deployment, the metadata table will return no nodes.
+				 * In this case, PgBouncer will not crash. Instead, it will disable the fast switchover feature and revert to the standard PgBouncer behavior.
+				 */
+				fast_switchover = false;
+				is_topology_monitoring_job_enabled = false;
+				log_warning("topology_query did not find at least 2 nodes to use Blue Green Deployment fast switchover for DB: '%s'. PgBouncer reverting to standard connection pooling mode.", server->pool->db->name);
+			} else {
+				fatal("topology_query did not find at least 2 nodes to use fast switchover in DB: '%s'. Is the topology table populated with entries?", server->pool->db->name);
+			}
+		}
+		server->pool->initial_writer_endpoint = false;
+
 		/* login ok */
 		slog_debug(server, "server login ok, start accepting queries");
 		server->ready = true;
@@ -287,6 +516,20 @@ int pool_pool_size(PgPool *pool)
 /* min_pool_size of the pool's db */
 int pool_min_pool_size(PgPool *pool)
 {
+	PgPool *global_writer;
+	if (fast_switchover && pool->db->blue_green_deployment_db_type == BLUE_GREEN_TARGET) {
+		/*
+		 * In a blue-green deployment during a fast switchover to switch the connection to green:
+		 *   - We maintan the pool size as global writer active connections
+		 *   - If global writer found, return the total number of active server connections
+		 *     currently associated with that writer pool.
+		 */
+		global_writer = get_global_writer(pool);
+		if (global_writer) {
+			return global_writer->active_server_list.cur_count;
+		}
+	}
+
 	return database_min_pool_size(pool->db);
 }
 
@@ -302,6 +545,14 @@ usec_t pool_server_lifetime(PgPool *pool)
 /* min_pool_size of the db */
 int database_min_pool_size(PgDatabase *db)
 {
+	if (fast_switchover && db->blue_green_deployment_db_type == BLUE_GREEN_SOURCE) {
+		/*
+		 *
+		 * In a blue-green deployment during a fast switchover:
+		 *   - For Source instance pool maintain only one connection to run the topolgy query
+		*/
+		return 1;
+	}
 	if (db->min_pool_size < 0)
 		return cf_min_pool_size;
 	else
@@ -352,6 +603,225 @@ int user_client_max_connections(PgGlobalUser *user)
 		return user->max_user_client_connections;
 }
 
+/*
+ * Returns true if global writer is lost and need to update the global writer to accept writes.
+ *
+ * data: result from one of the three queries:
+ *   In case of Multi_AZ Cluster - SELECT pg_is_in_recovery(); -> "f" or "t"
+ *   In case of Blue Green Deployment of Instance - SELECT status FROM rds_tools.show_topology('pgbouncer');
+ *   In case of Blue Green Deployment of Cluster - SELECT status FROM get_blue_green_fast_switchover_metadata(\'pgbouncer\');
+ * db_type: the database type (e.g., DEFAULT) of the current pool
+ */
+
+static bool should_update_global_writer(const char *data, enum BlueGreenDeploymentDBType db_type) {
+	return	(strcmp(data, "f") == 0) ||
+			(strcmp(data, SWITCHOVER_IN_POST_PROCESSING) == 0) ||
+			(strcmp(data, SWITCHOVER_COMPLETED) == 0);
+}
+
+static bool find_target_endpoint_to_setup_connection(PgSocket *server, TopologyData *topology) {
+	// If the role is still source — skip processing
+	if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0) {
+		return false;
+	}
+	/*
+	 * Blue-Green Deployment Fast Switchover:
+	 * Open connections only to the specified endpoint and its green counterpart; skip all other endpoints.
+	 * Connection Pool Handling:
+	 * - For writer endpoints (e.g., used with PgBouncer), ensure a connection is established to the corresponding writer on the green cluster.
+	 * - For reader endpoints, establish a connection to the equivalent reader on the green cluster.
+	 * - Apply the same logic for custom-defined endpoints.
+	 */
+	return setup_connection_pool_based_on_endpoint_type(server->pool->db->host, &topology->endpoint);
+}
+
+/*
+ * Free all entries in the failed_pools list and the list itself.
+ * Safe to call with NULL.
+ */
+static void cleanup_failed_pool_entry(struct StatList *failed_pools) {
+	struct List *item, *tmp;
+	if (failed_pools) {
+		// Cleanup
+		statlist_for_each_safe(item, failed_pools, tmp) {
+			struct FailedPgPool *entry = container_of(item, struct FailedPgPool, head);
+			statlist_remove(failed_pools, item);
+			free(entry);
+		}
+		free(failed_pools);
+	}
+}
+
+/**
+ * check_target_pools_with_failed_connection - Identify target pools with failed connections or mismatched configuration.
+ *
+ * This function iterates over the list of configured connection pools and checks for
+ * those that are designated as "target" in a blue-green deployment. It identifies
+ * target pools that meet any of the following failure criteria:
+ *   - The last connection attempt failed (`last_connect_failed` is true),
+ *   - The port number in the pool configuration differs from the current topology port,
+ *   - The host name in the pool configuration does not match the current topology endpoint.
+ *
+ * For each target pool that matches one or more of these failure conditions, a
+ * `FailedPgPool` entry is created and added to a `StatList` of failed pools. This
+ * allows the system to later act on unhealthy or outdated connections during
+ * deployment or failover operations.
+ *
+ * Memory for the returned `StatList` is dynamically allocated and must be freed by
+ * the caller. If memory allocation fails at any point, the function logs an error
+ * and returns NULL.
+ *
+ * @param topology Pointer to the current deployment topology containing expected host and port.
+ * @return A pointer to a `StatList` containing failed target pools, or NULL on failure.
+ */
+
+static struct StatList* check_target_pools_with_failed_connection(TopologyData *topology) {
+    struct List *item;
+    PgPool *next_pool = NULL;
+    struct StatList *failed_pools = NULL;
+
+    // Initialize the list to store failed pools
+    failed_pools = malloc(sizeof(struct StatList));
+    if (!failed_pools) {
+		log_error("Failed to allocate memory for failed_pools list");
+		return NULL;
+    }
+
+    statlist_init(failed_pools, "failed_pools");
+
+    // Look through pool list for target instance's pool
+    statlist_for_each(item, &pool_list) {
+		next_pool = container_of(item, PgPool, head);
+		if (next_pool->db->blue_green_deployment_db_type == BLUE_GREEN_TARGET) {
+			// Check if instance is unhealthy or has configuration changes
+			if (next_pool->last_connect_failed ||
+				next_pool->db->port != topology->port_num ||
+				strcmp(next_pool->db->host, topology->endpoint) != 0) {
+
+				struct FailedPgPool *entry = malloc(sizeof(struct FailedPgPool));
+				if (!entry) {
+					log_error("Failed to allocate memory for FailedPgPool entry");
+					cleanup_failed_pool_entry(failed_pools);
+					return NULL;
+				}
+				list_init(&entry->head);
+				entry->pool = next_pool;
+				// Add to failed_pools list using the existing head member of PgPool
+				statlist_append(failed_pools, &entry->head);
+			}
+		}
+	}
+	return failed_pools;
+}
+
+/*
+ * Check if a new pool is required for the Blue-Green target instance
+ * based on changes in host or port.
+ */
+static bool check_if_new_pool_required_for_target(PgPool *target_instance_pool, TopologyData *topology) {
+	// Check if hostname or port has changed
+    if (strcmp(target_instance_pool->db->host, topology->endpoint) != 0) {
+		// Kill existing pool for outdated target
+		log_debug("handle_server_work: Detected a change in the green DB host. "
+			"Killing the existing database connection. db_name: %s, old_host: %s, new_host: %s",
+			target_instance_pool->db->name, target_instance_pool->db->host, topology->endpoint);
+		return true;
+	} else if (target_instance_pool->db->port != topology->port_num) {
+		// Update the port for the pool before opening the connection
+		log_debug("handle_server_work: Detected a change in the green DB host. "
+			"Updating the port for db_name: %s, old_port: %d, new_port: %d",
+			target_instance_pool->db->name, target_instance_pool->db->port, topology->port_num);
+		updated_target_port_num = topology->port_num;
+		target_instance_pool->db->port = topology->port_num;
+	}
+
+	log_debug("handle_server_work: launch new connection for the db : %s", target_instance_pool->db->name);
+	// reuse existing pool
+	launch_new_connection(target_instance_pool, true);
+	return false;
+}
+
+/*
+ * Attempts to create and initialize a new connection pool for the updated topology.
+ *
+ * This function calls setup_new_pool() with the given server and topology data.
+ * - Returns true if the new pool is successfully created and initialized.
+ * - Returns false if pool creation fails, logging the failure reason.
+ *
+ * On success, the new pool inherits the parent pool from the server's existing pool.
+ */
+
+
+static bool initialize_new_target_pool(PgPool *parent_pool, TopologyData *topology) {
+	PgPool *new_pool = setup_new_pool(parent_pool, topology/* for updated target pool connection*/);
+	if (new_pool == NULL) {
+		log_debug("handle_server_work: Failed to initialize new connection pool for host '%s'. Possible issue with memory allocation, hostname parsing, or connection launch.",
+			topology->endpoint);
+		return false;
+	}
+	log_debug("handle_server_work: Successfully initialized new database connection. db_name: %s, port number: %d", new_pool->db->name, new_pool->db->port);
+	return true;
+}
+
+/**
+ * handle_failed_target_pools - Replaces failed or outdated target pools with updated ones.
+ *
+ * This function iterates over a list of failed target pools (typically identified by
+ * a mismatch in endpoint or failed connection attempts) and determines whether a new
+ * pool needs to be created for each entry using the current topology information.
+ *
+ * For each failed pool that requires replacement:
+ *   - It logs the original database name and host.
+ *   - Extracts the base database name by stripping the endpoint suffix.
+ *   - Destroys the existing (failed) database connection using `kill_database`.
+ *   - Creates a new target pool with updated host/port/topology using `initialize_new_target_pool`.
+ *
+ * If any part of this process fails (e.g., memory allocation, base name extraction, or
+ * pool initialization), the function logs an error and returns false to indicate that
+ * the operation was unsuccessful. On success, it returns true.
+ *
+ * The caller must ensure that `failed_pools` is valid and correctly populated before calling.
+ *
+ * @param failed_pools  List of target pools that have failed or become outdated.
+ * @param topology      The current deployment topology containing expected endpoint and port.
+ *
+ * @return true on successful replacement of all failed pools; false if any step fails.
+ */
+
+static bool handle_failed_target_pools(struct StatList *failed_pools, TopologyData *topology) {
+	struct List *item;
+	PgPool *target_instance_pool;
+	PgPool *parent_pool;
+
+	statlist_for_each(item, failed_pools) {
+		// Get the PgPool from the item's next pointer which points to PgPool's head
+		struct FailedPgPool *failed = container_of(item, struct FailedPgPool, head);
+		target_instance_pool = failed->pool;
+
+		if (check_if_new_pool_required_for_target(target_instance_pool, topology)) {
+			parent_pool = target_instance_pool->parent_pool;
+			kill_database(target_instance_pool->db);
+			parent_pool->num_nodes--;
+
+			// Set up a new pool with the updated endpoint and topology information
+			if (!initialize_new_target_pool(parent_pool, topology)) {
+				log_error("Failed to set up connection pool. Aborting to prevent inconsistent topology state.");
+				return false;
+			}
+		}
+	}
+	return true;
+}
+
+static bool cleanup_data_and_topology(char *data, TopologyData *topology, SBuf *sbuf, PktHdr *pkt, struct StatList *failed_pools) {
+	free(data);
+	cleanup_topology(topology);
+	cleanup_failed_pool_entry(failed_pools);
+	sbuf_prepare_skip(sbuf, pkt->len);
+	return true;
+}
+
+
 /* process packets on logged in connection */
 static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 {
@@ -363,6 +833,13 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 	bool async_response = false;
 	struct List *item, *tmp;
 	bool ignore_packet = false;
+	bool res = false;
+	PgPool *next_pool = NULL;
+	TopologyData *topology = NULL;
+	const char *topology_parser_msg;
+	char *data = NULL;
+	PgDatabase *db = NULL;
+	struct StatList *failed_pools = NULL;
 
 	Assert(!server->pool->db->admin);
 
@@ -374,6 +851,15 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 
 	/* pooling decisions will be based on this packet */
 	case PqMsg_ReadyForQuery:
+		/*
+		 * Discard topology data without sending to the client is finished. Resume regular
+		 * client/server communication.
+		 */
+		if (fast_switchover && server->pool->db->topology_query && server->pool->collect_datarows) {
+			server->pool->collect_datarows = false;
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		}
 
 		/* if partial pkt, wait */
 		if (!mbuf_get_char(&pkt->data, &state))
@@ -461,6 +947,39 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 		break;
 
 	case PqMsg_CommandComplete:
+		/*
+		 * In the process of discarding topology data without sending to the client.
+		 */
+		if (server->pool->collect_datarows) {
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		} else if (fast_switchover && server->pool->db->recovery_query && server->pool->checking_for_new_writer && server->state != SV_TESTED) {
+			/*
+			 * This handles a corner case where the recovery query did not return any data rows for any reason.
+			 * To keep searching for the new writer, we set `checking_for_new_writer` to false for this pool,
+			 * allowing the recovery query to run again through this pool.
+			 */
+			log_debug("handle_server_work: switchover started retrying to get the state of Blue Green deployment, db_name: %s", server->pool->db->name);
+			server->pool->checking_for_new_writer = false;
+		} else if (fast_switchover && server->pool->db->is_topology_check_in_progress && is_topology_monitoring_job_enabled) {
+			/*
+			 * Topology query execution completed, but PqMsg_DataRow handler wasn't triggered.
+			 * This means the topology query returned no results, indicating the
+			 * topology table is empty. Since fast switchover requires topology
+			 * data to function properly, so we must disable this feature now.
+			 */
+			log_warning("handle_server_work: topology query returned no results for db: %s, Disabling fast switchover.", server->pool->db->name);
+			server->pool->db->is_topology_check_in_progress = false;
+			fast_switchover = false;
+			is_topology_monitoring_job_enabled = false;
+			statlist_for_each_safe(item, &database_list, tmp) {
+				db = container_of(item, PgDatabase, head);
+				if (db->blue_green_deployment_db_type == BLUE_GREEN_TARGET) {
+					kill_database(db);
+				}
+			}
+		}
+
 		/* ErrorResponse and CommandComplete show end of copy mode */
 		if (server->copy_mode) {
 			slog_debug(server, "COPY finished");
@@ -545,6 +1064,110 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 	/* data packets, there will be more coming */
 	case PqMsg_CopyData:
 	case PqMsg_DataRow:
+		/*
+		 * These are the rows returned from the topology query that are discarded.
+		 */
+		if (server->pool->collect_datarows) {
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		} else if (fast_switchover && server->pool->checking_for_new_writer && server->state != SV_TESTED) {
+			char *data = query_data(pkt);
+			/*
+			 * Once the switchover begins, the recovery query will run on the target instance to retrieve the switchover status from the metadata table.
+			 * When the status changes to either SWITCHOVER_IN_POST_PROCESSING or SWITCHOVER_COMPLETED,
+			 * it indicates that the target instance is ready for accepting the writes, and we can safely update the global writer to point to the target instance.
+			 *
+			 * If the recovery query ran on a DEFAULT database type, and the topology data shows the Blue-Green deployment is in an AVAILABLE state,
+			 * that means the global writer was lost because the green (target) instance went down. So reset the global writer to the default database pool (server->pool)
+			 */
+			if (should_update_global_writer(data, server->pool->db->blue_green_deployment_db_type)) {
+				log_debug("handle_server_work: connected to writer (pg_is_in_recovery is '%s'): db_name %s", data, server->pool->db->name);
+
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+				// new writer has been found, so indicate that we need to refresh the topology.
+				server->pool->refresh_topology = true;
+				if (server->pool->db->recovery_query && server->pool->db->blue_green_deployment_db_type == BLUE_GREEN_TARGET) {
+					/*
+					 * At this point, we've run the recovery query on the green (target) instance.
+					 * The metadata table shows 'SWITCHOVER_IN_POST_PROCESSING', which meanss
+					 * green instance is now ready to accept write operations.
+					 */
+					routing_traffic_to_target_complete =  true;
+					is_topology_monitoring_job_enabled = false;
+				}
+			} else if (server->pool->db->blue_green_deployment_db_type == BLUE_GREEN_TARGET && strcmp(data, AVAILABLE) == 0) {
+				/*
+				 * This scenario occurs when the source instance becomes unavailable before the switchover.
+				 * As a result, the switchover status entry in the Blue-Green Deployment metadata table remains as AVAILABLE.
+				 * In this case, we will attempt to re-establish the closed connection and set the source_instance_down_before_switchover flag to true.
+				 */
+				log_debug("handle_server_work: source instance is down before the switchover need to open the new connection, recovery query ran on : db_name %s", server->pool->db->name);
+				source_instance_down_before_switchover = true;
+			} else {
+				log_debug("handle_server_work: connected to reader (pg_is_in_recovery is '%s'). db_name: %s, Must keep polling until next server.", data, server->pool->db->name);
+			}
+			server->pool->checking_for_new_writer = false;
+			free(data);
+		} else if (fast_switchover && server->pool->db->is_topology_check_in_progress) {
+			/*
+			 * If the target instance pool is in a failed state,
+			 * 1. Parse the topology data from the current packet.
+			 * 2. If the role is BLUE_GREEN_DEPLOYMENT_SOURCE, skip processing the packet.
+			 * 3. Allocate memory for the hostname.
+			 * 4. Set up the connection pool based on the endpoint type.
+			 * 5. If the port has changed for the target pool, update the port num
+			 * 6. If the hostname has changed, kill the existing database.
+			 * 7. Create a new pool using the updated topology data.
+			 * 8. Set the new pool's parent pool to the server's parent pool.
+			 * 9. Open a new connection for the target pool
+			 */
+			data = query_data(pkt);
+			// Parse the topology data received from the metadata query
+			topology_parser_msg = parse_topology_data(server, data, &topology);
+			if (topology_parser_msg) {
+				free(data);
+				fatal("failed while parsing the topology data. Error msg : %s", topology_parser_msg);
+			}
+
+			// During Switchover do not let the topology job to perform any action
+			if (strcmp(topology->status, AVAILABLE) != 0) {
+				server->pool->db->is_topology_check_in_progress = false;
+				return cleanup_data_and_topology(data, topology, sbuf, pkt, failed_pools);
+			}
+
+			if (!find_target_endpoint_to_setup_connection(server, topology)) {
+				return cleanup_data_and_topology(data, topology, sbuf, pkt, failed_pools);
+			}
+
+			failed_pools = check_target_pools_with_failed_connection(topology);
+			if (failed_pools == NULL) {
+				cleanup_topology(topology);
+				free(data);
+				fatal("Unable to extract target pools list with failed connections from topology. "
+					"This could be due to memory allocation failure or no matching pools found.");
+			}
+
+			// All Target resource has healthy connection, skipping the recovery
+			if (statlist_empty(failed_pools)) {
+				server->pool->db->is_topology_check_in_progress = false;
+				return cleanup_data_and_topology(data, topology, sbuf, pkt, failed_pools);
+			}
+
+			if(!handle_failed_target_pools(failed_pools, topology)) {
+				cleanup_topology(topology);
+				free(data);
+				cleanup_failed_pool_entry(failed_pools);
+				fatal("Failed to handle and update target pools with connection issues or configuration mismatches. "
+					"This could be due to inability to extract base database names, failure in terminating existing connections, "
+					"or problems initializing new pools with updated topology. "
+					"Aborting to prevent inconsistent topology state and potential data integrity issues.");
+			}
+			server->pool->db->is_topology_check_in_progress = false;
+			return cleanup_data_and_topology(data, topology, sbuf, pkt, failed_pools);
+		}
 		break;
 	}
 	server->idle_tx = idle_tx;
@@ -632,7 +1255,7 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 			}
 		}
 	} else {
-		if (server->state != SV_TESTED) {
+		if (server->state != SV_TESTED && !server->pool->db->topology_query) {
 			slog_warning(server,
 				     "got packet '%c' from server when not linked",
 				     pkt_desc(pkt));
@@ -640,6 +1263,19 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 		sbuf_prepare_skip(sbuf, pkt->len);
 	}
 
+	/*
+	 * pg_is_in_recovery() finished since we received a ReadyForQuery and refresh_topology is set.
+	 * Mark the pool as needing to collect data rows and send the topology query to the writer.
+	 */
+	if (server->pool->refresh_topology && pkt->type == 'Z') {
+		server->pool->collect_datarows = true;
+		server->pool->refresh_topology = false;
+
+		SEND_generic(res, server, PqMsg_Query, "s", server->pool->db->topology_query);
+		if (!res)
+			disconnect_server(server, false, "exec_on_connect query failed");
+		}
+
 	return true;
 }
 
@@ -754,6 +1390,7 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 	bool res = false;
 	PgSocket *server = container_of(sbuf, PgSocket, sbuf);
 	PgPool *pool = server->pool;
+	PgPool *global_writer = get_global_writer(pool);
 	PktHdr pkt;
 	char infobuf[96];
 
@@ -768,8 +1405,18 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 	case SBUF_EV_RECV_FAILED:
 		if (server->state == SV_ACTIVE_CANCEL)
 			disconnect_server(server, false, "successfully sent cancel request");
-		else
+		else {
+			if (global_writer && pool->db->blue_green_deployment_db_type != BLUE_GREEN_TARGET)
+				clear_global_writer(pool);
+
+			// mark main pool as failed as well (the writer)
+			if (fast_switchover) {
+				pool->last_failed_time = get_cached_time();
+				pool->last_connect_failed = true;
+				server->pool->checking_for_new_writer = false;
+			}
 			disconnect_server(server, false, "server conn crashed?");
+		}
 		break;
 	case SBUF_EV_SEND_FAILED:
 		disconnect_client(server->link, false, "unexpected eof");
@@ -810,6 +1457,10 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 		break;
 	case SBUF_EV_CONNECT_FAILED:
 		Assert(server->state == SV_LOGIN);
+		if (fast_switchover) {
+			pool->last_failed_time = get_cached_time();
+			server->pool->checking_for_new_writer = false;
+		}
 		disconnect_server(server, false, "connect failed");
 		break;
 	case SBUF_EV_CONNECT_OK:
@@ -893,3 +1544,289 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 		takeover_login_failed();
 	return res;
 }
+
+/**
+ * @brief Parse and validate topology data from a query result.
+ *
+ * This function parses the raw tab-separated data from a topology query into a
+ * dynamically allocated TopologyData structure. It performs validation checks
+ * based on the deployment configuration, particularly for Blue-Green setups.
+ *
+ * @param server        The PgSocket associated with the query context.
+ * @param data          The raw string data returned from the topology query.
+ * @param topology_out  Output parameter; on success, points to the populated TopologyData structure.
+ *
+ * @return NULL on success, or a static error message string if parsing or validation fails.
+ *
+ * @note This function performs the following steps:
+ *       1. Allocates memory for the TopologyData structure.
+ *       2. Parses the input string into endpoint, role, port, and status fields.
+ *       3. Validates the parsed data based on the server's deployment configuration.
+ *       4. Converts the port string to an integer.
+ *       5. Enforces rules specific to Blue-Green deployments.
+ *
+ * @attention The caller is responsible for:
+ *            - Logging or handling the returned error message, if any.
+ *            - Freeing the allocated TopologyData structure on success.
+ *
+ * @warning This function no longer calls fatal() internally. Instead, it returns
+ *          descriptive error strings on failure, allowing the caller to decide how
+ *          to handle errors.
+ *
+ * @see TopologyData
+ * @see cleanup_topology()
+ * @see BLUE_GREEN_DEPLOYMENT_TARGET
+ * @see BLUE_GREEN_DEPLOYMENT_SOURCE
+ */
+const char* parse_topology_data(PgSocket *server, char *data, TopologyData **topology_out) {
+    TopologyData *topology = malloc(sizeof(TopologyData));
+    if (!topology) {
+		return "malloc failed: no memory for topology data";
+	}
+
+	memset(topology, 0, sizeof(TopologyData)); // Initialize to zero
+
+	// Make a copy of data since strsep modifies it
+	char *data_copy = strdup(data);
+	if (!data_copy) {
+		cleanup_topology(topology);
+		return "strdup failed: no memory for data copy";
+
+	}
+
+	char *pointer_to_data = data_copy;
+	char *token;
+	char *port_str = NULL;
+
+	// Parse endpoint
+	token = strsep(&pointer_to_data, "\t");
+	topology->endpoint = token ? strdup(token) : NULL;
+	if (!topology->endpoint) {
+		cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+		return "strdup failed: no memory for topology->endpoint";
+	}
+
+	// Parse role
+	token = strsep(&pointer_to_data, "\t");
+	if (token) {
+		topology->role = strdup(token);
+		if (!topology->role) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "strdup failed: no memory for topology->role";
+		}
+	}
+
+	// Parse port
+	token = strsep(&pointer_to_data, "\t");
+	if (token) {
+		port_str = strdup(token);
+		if (!port_str) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "strdup failed: no memory for port_str";
+		}
+	}
+
+	// Parse status
+	token = strsep(&pointer_to_data, "\t");
+	if (token) {
+		topology->status = strdup(token);
+		if (!topology->status) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "strdup failed: no memory for topology->status";
+		}
+	}
+
+	if (server->pool->db->recovery_query) {
+		/*
+		 * In a Blue-Green deployment, the port and role are retrieved using the topology query.
+		 * When PgBouncer is configured with the fast switchover feature enabled for Blue-Green deployment,
+		 * it expects valid role and port values. If valid values are not found, PgBouncer will crash.
+		 */
+		if (topology->role == NULL || port_str == NULL || topology->status == NULL) {
+			log_debug("port, role and status for Blue Green fast switchover, port: %s, role: %s, status: %s",
+				port_str ? port_str : "NULL", topology->role ? topology->role : "NULL", topology->status ? topology->status : "NULL");
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "port, role or status cannot be null while using the Blue Green fast switchover";
+		}
+
+		/*
+		 * This check handles the scenario where the customer has configured the blue-green deployment fast switch feature
+		 * but the endpoint is not part of a blue-green deployment.
+		 * In such cases, we intentionally crash PgBouncer, as fast switchovers are not supported in other deployment topologies.
+		 */
+		if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) != 0 &&
+			strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) != 0) {
+			log_debug("The current role for your endpoint is: %s", topology->role);
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "blue-green deployment fast switchover is only supported for BLUE_GREEN_DEPLOYMENT_SOURCE and BLUE_GREEN_DEPLOYMENT_TARGET roles";
+		}
+
+		/*
+		 * In a Blue-Green deployment for a cluster, fast switchover only works with cluster endpoints
+		 * (reader, writer, or custom).
+		 * Make sure the user has configured cluster endpoints, not instance endpoints of the cluster.
+		 */
+		if (is_cluster_endpoint(topology->endpoint) && !is_cluster_endpoint(server->pool->db->host)) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "For a cluster, fast switchover during blue-green deployment only supports cluster endpoints "
+				"(reader, writer, or custom). It looks like an instance endpoint is configured instead. "
+				"Please update the configuration to use a valid cluster endpoint.";
+		}
+
+		topology->port_num = atoi(port_str);
+		if (topology->port_num <= 0) {
+			log_debug("port number: %s", port_str);
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "Invalid port number in topology metadata table";
+		}
+
+		/*
+		 * This check handles the case where the customer has configured the blue-green deployment fast switch feature
+		 * but is using the green endpoint.
+		 * In this scenario, fast switchovers are not supported. Therefore, we update the customer with the appropriate message.
+		 */
+		if ((strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0) &&
+			(strcmp(server->pool->db->host, topology->endpoint) == 0) &&
+			(strcmp(topology->status, AVAILABLE) == 0)) {
+			log_debug("Configured endpoint : %s", server->pool->db->host);
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "for blue green deployment target(green) end point cannot be configured to achieve fast switchover feature.";
+		}
+    } else {
+		/*
+		 * This case handles situations where the customer has not configured the recovery query
+		 * in a blue-green deployment setup.
+		 */
+		if (topology->role && (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0 ||
+			strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0)) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "recovery query is mandatory for the blue-green deployment fast switchover feature, but it is currently missing.";
+		}
+		topology->port_num = server->pool->db->port;
+	}
+
+	log_debug("got initial data, endpoint: %s, role: %s, port: %d, status: %s",
+		topology->endpoint ? topology->endpoint : "NULL",
+		topology->role ? topology->role : "NULL",
+		topology->port_num,
+		topology->status ? topology->status : "NULL");
+
+	free(port_str);
+	free(data_copy);
+	*topology_out = topology;
+	return NULL;
+}
+
+/**
+ * @brief Set up a new connection pool based on topology data.
+ *
+ * This function creates and configures a new PgPool instance using the provided topology data.
+ *
+ * @param pool_to_configure The pool associated with the current connection.
+ * @param topology A pointer to the TopologyData structure containing parsed topology information.
+ *
+ * @return A pointer to the newly created PgPool if successful, or NULL if an error occurred.
+ *
+ * @note This function performs the following steps:
+ *       1. Validates and parses the hostname from the topology endpoint.
+ *       2. Creates a new pool using new_pool_from_db().
+ *       3. Sets up parent pool relationships and global writer.
+ *       4. Copies topology and recovery queries from the original pool.
+ *       5. Sets the Blue-Green deployment type based on the topology role.
+ *       6. Launches a new connection for the pool.
+ *       7. Increments the node count in the server's pool.
+ *
+ * @warning This function calls fatal() if it cannot parse the hostname, which may terminate the program.
+ *
+ * @warning The function assumes that the input parameters are valid and have been properly allocated.
+ *          It's the caller's responsibility to free the TopologyData and hostname after this function returns.
+ *
+ * @see new_pool_from_db()
+ * @see launch_new_connection()
+ */
+PgPool* setup_new_pool(PgPool *pool_to_configure, TopologyData *topology) {
+	/* Make a copy of endpoint for hostname */
+	char* hostname = strdup(topology->endpoint);
+	char* dbname_with_endpoint = NULL;
+	PgPool *new_pool = NULL;
+	if (hostname == NULL) {
+		log_warning("strdup: no mem for hostname");
+		return NULL;
+	}
+	if (!strtok(topology->endpoint, ".")) {
+		log_warning("could not parse hostname present in topology, hostname to parse: %s", topology->endpoint);
+		free(hostname);
+		return NULL;
+	}
+
+	dbname_with_endpoint = concat_multiple_strings(3, pool_to_configure->db->name, "_", topology->endpoint);
+
+	if (!dbname_with_endpoint) {
+		log_warning("Memory allocation failed while joining dbname and endpoint name");
+		free(hostname);
+		return NULL;
+	}
+
+	new_pool = new_pool_from_db(pool_to_configure->db, dbname_with_endpoint, hostname, topology->port_num);
+    if (!new_pool) {
+		free(dbname_with_endpoint);
+		free(hostname);
+		return NULL;
+    }
+
+	new_pool->parent_pool = pool_to_configure;
+	new_pool->parent_pool->global_writer = pool_to_configure;
+	new_pool->db->topology_query = strdup(pool_to_configure->db->topology_query);
+	if (new_pool->db->topology_query == NULL) {
+		log_warning("strdup: no mem for topology_query");
+		free(dbname_with_endpoint);
+		free(hostname);
+		return NULL;
+	}
+
+    if (pool_to_configure->db->recovery_query) {
+		new_pool->db->recovery_query = strdup(pool_to_configure->db->recovery_query);
+		if (new_pool->db->recovery_query == NULL) {
+			log_warning("strdup: no mem for recovery_query");
+			free(dbname_with_endpoint);
+			free(hostname);
+			return NULL;
+		}
+		if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0) {
+			new_pool->db->blue_green_deployment_db_type = BLUE_GREEN_SOURCE;
+		} else if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0) {
+			new_pool->db->blue_green_deployment_db_type = BLUE_GREEN_TARGET;
+		}
+    }
+
+	launch_new_connection(new_pool, true);
+	pool_to_configure->num_nodes++;
+	free(dbname_with_endpoint);
+	free(hostname);
+	return new_pool;
+}
+
+void cleanup_parsing_data_and_exit(TopologyData *topology, char *data_copy, char *port_str) {
+	cleanup_topology(topology);
+	if (data_copy) {
+		free(data_copy);
+	}
+	if (port_str) {
+		free(port_str);
+	}
+}
+
+void cleanup_topology(TopologyData *topology) {
+	if (topology) {
+		if (topology->endpoint) {
+			free(topology->endpoint);
+		}
+		if (topology->role) {
+			free(topology->role);
+		}
+		if (topology->status) {
+			free(topology->status);
+		}
+		free(topology);
+	}
+}
